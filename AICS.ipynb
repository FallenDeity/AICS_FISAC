{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1qtv0PjvV_v",
        "outputId": "b35d1299-5dde-46b6-d26b-4957981d4259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['datasets', '.ipynb_checkpoints', 'requirements.txt', 'network_threats.db']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"drive/MyDrive/aics\")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "558EVrNww3B8",
        "outputId": "02f031e7-9ad2-47f4-b95b-0b7a2b351dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting asttokens (from -r requirements.txt (line 1))\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting colorama (from -r requirements.txt (line 2))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting comm (from -r requirements.txt (line 3))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: contourpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: debugpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.4.2)\n",
            "Collecting exceptiongroup (from -r requirements.txt (line 8))\n",
            "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting executing (from -r requirements.txt (line 9))\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: fonttools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (4.57.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.17.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (7.34.0)\n",
            "Collecting jedi (from -r requirements.txt (line 13))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (1.4.2)\n",
            "Requirement already satisfied: jupyter_client in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (6.1.12)\n",
            "Requirement already satisfied: jupyter_core in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (5.7.2)\n",
            "Requirement already satisfied: kiwisolver in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (3.10.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (0.1.7)\n",
            "Requirement already satisfied: narwhals in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (1.34.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (1.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (2.2.2)\n",
            "Requirement already satisfied: parso in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (0.8.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (11.1.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (4.3.7)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (5.24.1)\n",
            "Requirement already satisfied: prompt_toolkit in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (3.0.50)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (5.9.5)\n",
            "Collecting pure_eval (from -r requirements.txt (line 32))\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (2.18.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (3.2.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (2025.2)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (24.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (1.14.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (0.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (1.17.0)\n",
            "Collecting stack-data (from -r requirements.txt (line 41))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (3.6.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (6.4.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (5.7.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (4.13.1)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (2025.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 47)) (0.2.13)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 12)) (75.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 12)) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 12)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 12)) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter_client->-r requirements.txt (line 15)) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->-r requirements.txt (line 29)) (9.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 12)) (0.7.0)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pure_eval, jedi, executing, exceptiongroup, comm, colorama, asttokens, stack-data\n",
            "Successfully installed asttokens-3.0.0 colorama-0.4.6 comm-0.2.2 exceptiongroup-1.2.2 executing-2.2.0 jedi-0.19.2 pure_eval-0.2.3 stack-data-0.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import sqlite3\n",
        "import ipaddress\n",
        "import joblib\n",
        "from contextlib import closing\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, log_loss, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, learning_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, label_binarize\n",
        "from sklearn.utils import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import networkx as nx\n",
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "osKe5Ka01RbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = [\n",
        "    \"Flow Duration\",\n",
        "    \"Total Fwd Packets\",\n",
        "    \"Total Backward Packets\",\n",
        "    \"out_degree\",\n",
        "    \"in_degree\",\n",
        "    \"active_days\",\n",
        "    \"activity_span\",\n",
        "    \"degree_centrality\",\n",
        "    \"days_since_last_seen\",\n",
        "    \"source_ip\",\n",
        "    \"destination_ip\",\n",
        "    \"Source Port\",\n",
        "    \"Destination Port\",\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    \"Protocol\",\n",
        "    \"Hour\",\n",
        "    \"DayOfWeek\",\n",
        "    \"Source_Internal\",\n",
        "    \"Dest_Internal\",\n",
        "]\n",
        "\n",
        "# Load data\n",
        "DATASET_DIR = pathlib.Path(\"datasets\")\n",
        "csv_files = list(DATASET_DIR.glob(\"*.csv\"))\n",
        "DB_NAME = \"network_threats.db\""
      ],
      "metadata": {
        "id": "hxsCJU-v1kiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(files):\n",
        "    \"\"\"Load CSV files into a list of DataFrames.\"\"\"\n",
        "    dfs = []\n",
        "    header = None\n",
        "\n",
        "    for f in files:\n",
        "        print(f\"Loading {f}...\")\n",
        "        try:\n",
        "            df = pd.read_csv(f, low_memory=False, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
        "            # Check if the header is consistent across files\n",
        "            if not header:\n",
        "                header = df.columns.tolist()\n",
        "            elif header != df.columns.tolist():\n",
        "                print(f\"Header mismatch in {f}. Skipping this file.\")\n",
        "                continue\n",
        "            # drop the header row if it exists in the data\n",
        "            if df.iloc[0].tolist() == header:\n",
        "                df = df.iloc[1:]\n",
        "            # Append the dataframe to the list\n",
        "            dfs.append(df)\n",
        "            print(f\"Loaded {f} with {len(df)} rows.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {f}: {e}\")\n",
        "\n",
        "    # Concatenate all dataframes and set the header\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    df.columns = [h.strip() for h in header]  # Clean column names\n",
        "\n",
        "    # drop duplicates and nans\n",
        "    df = df.drop_duplicates()\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "f7t3fbXM1pSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    print(\"Starting preprocessing...\")\n",
        "    # Convert timestamp\n",
        "    print(\"Converting Timestamp...\")\n",
        "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
        "    print(\"Timestamp conversion complete.\")\n",
        "\n",
        "    # IP categorization\n",
        "    def is_internal(ip):\n",
        "        # check if ip address is string\n",
        "        if not isinstance(ip, str):\n",
        "            print(f\"Invalid IP address: {ip}\")\n",
        "            return 0\n",
        "        octets = list(map(int, ip.split(\".\")))\n",
        "        if (\n",
        "            (octets[0] == 10)\n",
        "            or (octets[0] == 172 and 16 <= octets[1] <= 31)\n",
        "            or (octets[0] == 192 and octets[1] == 168)\n",
        "        ):\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    print(\"Categorizing IP addresses...\")\n",
        "    # Feature engineering\n",
        "    df[\"Source_Internal\"] = df[\"Source IP\"].apply(is_internal)\n",
        "    df[\"Dest_Internal\"] = df[\"Destination IP\"].apply(is_internal)\n",
        "    df[\"Hour\"] = df[\"Timestamp\"].dt.hour\n",
        "    df[\"DayOfWeek\"] = df[\"Timestamp\"].dt.dayofweek\n",
        "    # convert IP addresses to integers\n",
        "    df[\"source_ip\"] = df[\"Source IP\"].apply(\n",
        "        lambda x: int(ipaddress.ip_address(x)) if isinstance(x, str) else x\n",
        "    )\n",
        "    df[\"destination_ip\"] = df[\"Destination IP\"].apply(\n",
        "        lambda x: int(ipaddress.ip_address(x)) if isinstance(x, str) else x\n",
        "    )\n",
        "    print(\"IP categorization complete.\")\n",
        "\n",
        "    print(\"Entering categorical columns...\")\n",
        "    label_encoder = LabelEncoder()\n",
        "    df[\"label_encoded\"] = label_encoder.fit_transform(df[\"Label\"])\n",
        "\n",
        "    print(\"Categorical columns entered.\")\n",
        "    return df, label_encoder"
      ],
      "metadata": {
        "id": "daLM3ZMA1vBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_database():\n",
        "    \"\"\"Create SQLite database schema\"\"\"\n",
        "    with closing(sqlite3.connect(DB_NAME)) as conn:\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Create main flows table\n",
        "        cursor.execute(\n",
        "            \"\"\"CREATE TABLE IF NOT EXISTS flows (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            timestamp DATETIME,\n",
        "            src_ip TEXT,\n",
        "            src_port INTEGER,\n",
        "            dst_ip TEXT,\n",
        "            dst_port INTEGER,\n",
        "            protocol INTEGER,\n",
        "            duration REAL,\n",
        "            fwd_packets INTEGER,\n",
        "            bwd_packets INTEGER,\n",
        "            label INTEGER,\n",
        "            src_internal INTEGER,\n",
        "            dst_internal INTEGER\n",
        "        )\"\"\"\n",
        "        )\n",
        "\n",
        "        # Create IP metadata table\n",
        "        cursor.execute(\n",
        "            \"\"\"CREATE TABLE IF NOT EXISTS ip_metadata (\n",
        "            ip TEXT PRIMARY KEY,\n",
        "            internal INTEGER,\n",
        "            degree_centrality REAL,\n",
        "            last_seen DATETIME)\"\"\"\n",
        "        )\n",
        "\n",
        "        # Create indexes\n",
        "        cursor.execute(\"\"\"CREATE INDEX IF NOT EXISTS idx_src_ip ON flows(src_ip)\"\"\")\n",
        "        cursor.execute(\"\"\"CREATE INDEX IF NOT EXISTS idx_dst_ip ON flows(dst_ip)\"\"\")\n",
        "        conn.commit()\n",
        "\n",
        "\n",
        "def update_ip_metadata():\n",
        "    \"\"\"Update IP metadata using incremental steps\"\"\"\n",
        "    with closing(sqlite3.connect(DB_NAME)) as conn:\n",
        "        # Step 1: Create temp tables\n",
        "        conn.execute(\n",
        "            \"\"\"\n",
        "        CREATE TEMPORARY TABLE IF NOT EXISTS temp_src_ips AS\n",
        "        SELECT\n",
        "            src_ip AS ip,\n",
        "            MAX(src_internal) AS internal,\n",
        "            COUNT(*) AS out_degree,\n",
        "            MAX(timestamp) AS last_seen\n",
        "        FROM flows\n",
        "        GROUP BY src_ip\n",
        "        \"\"\"\n",
        "        )\n",
        "\n",
        "        conn.execute(\n",
        "            \"\"\"\n",
        "        CREATE TEMPORARY TABLE IF NOT EXISTS temp_dst_ips AS\n",
        "        SELECT\n",
        "            dst_ip AS ip,\n",
        "            MAX(dst_internal) AS internal,\n",
        "            COUNT(*) AS in_degree,\n",
        "            MAX(timestamp) AS last_seen\n",
        "        FROM flows\n",
        "        GROUP BY dst_ip\n",
        "        \"\"\"\n",
        "        )\n",
        "\n",
        "        # Step 2: Combine IP data\n",
        "        conn.execute(\n",
        "            \"\"\"\n",
        "            CREATE TEMPORARY TABLE IF NOT EXISTS temp_combined_ips AS\n",
        "            SELECT\n",
        "                ip,\n",
        "                MAX(internal) AS internal,\n",
        "                SUM(out_degree) AS out_degree,\n",
        "                SUM(in_degree) AS in_degree,\n",
        "                MAX(last_seen) AS last_seen\n",
        "            FROM (\n",
        "                SELECT ip, internal, out_degree, 0 AS in_degree, last_seen FROM temp_src_ips\n",
        "                UNION ALL\n",
        "                SELECT ip, internal, 0 AS out_degree, in_degree, last_seen FROM temp_dst_ips\n",
        "            )\n",
        "            GROUP BY ip\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Step 3: Update metadata table\n",
        "        conn.execute(\n",
        "            \"\"\"\n",
        "        INSERT OR REPLACE INTO ip_metadata\n",
        "        SELECT\n",
        "            ip,\n",
        "            internal,\n",
        "            (COALESCE(out_degree, 0) + (COALESCE(in_degree, 0))) AS degree_centrality,\n",
        "            last_seen\n",
        "        FROM temp_combined_ips\n",
        "        \"\"\"\n",
        "        )\n",
        "\n",
        "        # Step 4: Cleanup temp tables\n",
        "        conn.execute(\"DROP TABLE IF EXISTS temp_src_ips\")\n",
        "        conn.execute(\"DROP TABLE IF EXISTS temp_dst_ips\")\n",
        "        conn.execute(\"DROP TABLE IF EXISTS temp_combined_ips\")\n",
        "\n",
        "        conn.commit()\n",
        "\n",
        "\n",
        "def store_in_database(df):\n",
        "    \"\"\"Store preprocessed data in SQLite\"\"\"\n",
        "    flow_data = df.rename(\n",
        "        columns={\n",
        "            \"Timestamp\": \"timestamp\",\n",
        "            \"Source IP\": \"src_ip\",\n",
        "            \"Source Port\": \"src_port\",\n",
        "            \"Destination IP\": \"dst_ip\",\n",
        "            \"Destination Port\": \"dst_port\",\n",
        "            \"Protocol\": \"protocol\",\n",
        "            \"Flow Duration\": \"duration\",\n",
        "            \"Total Fwd Packets\": \"fwd_packets\",\n",
        "            \"Total Backward Packets\": \"bwd_packets\",\n",
        "            \"Label\": \"label\",\n",
        "            \"Source_Internal\": \"src_internal\",\n",
        "            \"Dest_Internal\": \"dst_internal\",\n",
        "        }\n",
        "    )\n",
        "    with closing(sqlite3.connect(DB_NAME)) as conn:\n",
        "        # Store flows\n",
        "        flow_data[\n",
        "            [\n",
        "                \"timestamp\",\n",
        "                \"src_ip\",\n",
        "                \"src_port\",\n",
        "                \"dst_ip\",\n",
        "                \"dst_port\",\n",
        "                \"protocol\",\n",
        "                \"duration\",\n",
        "                \"fwd_packets\",\n",
        "                \"bwd_packets\",\n",
        "                \"label\",\n",
        "                \"src_internal\",\n",
        "                \"dst_internal\",\n",
        "            ]\n",
        "        ].to_sql(\"flows\", conn, if_exists=\"append\", index=False)\n",
        "\n",
        "        conn.commit()\n",
        "\n",
        "        # Update IP metadata with latest information\n",
        "        update_ip_metadata()\n",
        "\n",
        "\n",
        "def calculate_graph_features(ip_filter=None):\n",
        "    \"\"\"Calculate features with data leakage prevention\"\"\"\n",
        "    with closing(sqlite3.connect(DB_NAME)) as conn:\n",
        "        # Degree features\n",
        "        degree_query = \"\"\"\n",
        "        SELECT ip,\n",
        "            SUM(out_degree) AS out_degree,\n",
        "            SUM(in_degree) AS in_degree\n",
        "        FROM (\n",
        "            SELECT src_ip AS ip, 1 AS out_degree, 0 AS in_degree FROM flows\n",
        "            UNION ALL\n",
        "            SELECT dst_ip AS ip, 0 AS out_degree, 1 AS in_degree FROM flows\n",
        "        )\n",
        "        GROUP BY ip\n",
        "        \"\"\"\n",
        "        degrees = pd.read_sql(degree_query, conn)\n",
        "\n",
        "        # Temporal features\n",
        "        temp_query = (\n",
        "            \"\"\"\n",
        "            SELECT\n",
        "                src_ip AS ip,\n",
        "                COUNT(DISTINCT DATE(timestamp)) AS active_days,\n",
        "                JULIANDAY(MAX(timestamp)) - JULIANDAY(MIN(timestamp)) AS activity_span\n",
        "            FROM flows\n",
        "            \"\"\"\n",
        "            + (\n",
        "                f\"WHERE src_ip IN ({','.join(['?']*len(ip_filter))})\"\n",
        "                if len(ip_filter)\n",
        "                else \"\"\n",
        "            )\n",
        "            + \"\"\"\n",
        "            GROUP BY src_ip\n",
        "            \"\"\"\n",
        "        )\n",
        "        params = tuple(ip_filter) if len(ip_filter) else ()\n",
        "        temporal = pd.read_sql(temp_query, conn, params=params)\n",
        "\n",
        "    return degrees.merge(temporal, on=\"ip\", how=\"left\").fillna(0)\n",
        "\n",
        "\n",
        "def enrich_data(df, graph_features):\n",
        "    \"\"\"Enrich with metadata from ip_metadata table\"\"\"\n",
        "    with closing(sqlite3.connect(DB_NAME)) as conn:\n",
        "        ip_meta = pd.read_sql(\n",
        "            \"\"\"\n",
        "            SELECT\n",
        "                ip,\n",
        "                degree_centrality,\n",
        "                JULIANDAY('now') - JULIANDAY(last_seen) AS days_since_last_seen\n",
        "            FROM ip_metadata\n",
        "            \"\"\",\n",
        "            conn,\n",
        "        )\n",
        "\n",
        "    # enhanced = (\n",
        "    #     df.merge(graph_features, left_on=\"Source IP\", right_on=\"ip\", how=\"left\")\n",
        "    #     .merge(ip_meta, left_on=\"Source IP\", right_on=\"ip\", how=\"left\")\n",
        "    #     .fillna({\"degree_centrality\": 0, \"days_since_last_seen\": 365})\n",
        "    # )\n",
        "\n",
        "    # return enhanced.drop(columns=[\"ip_x\", \"ip_y\"], errors=\"ignore\")\n",
        "\n",
        "    # merge in chunks\n",
        "    chunk_size = 10000\n",
        "    enriched_df = pd.DataFrame()\n",
        "    for i in range(0, len(df), chunk_size):\n",
        "        chunk = df.iloc[i : i + chunk_size]\n",
        "        chunk = (\n",
        "            chunk.merge(graph_features, left_on=\"Source IP\", right_on=\"ip\", how=\"left\")\n",
        "            .merge(ip_meta, left_on=\"Source IP\", right_on=\"ip\", how=\"left\")\n",
        "            .fillna({\"degree_centrality\": 0, \"days_since_last_seen\": 365})\n",
        "        )\n",
        "        enriched_df = pd.concat([enriched_df, chunk], ignore_index=True)\n",
        "\n",
        "        print(f\"Processed chunk {i // chunk_size + 1}\")\n",
        "\n",
        "    return enriched_df.drop(columns=[\"ip_x\", \"ip_y\"], errors=\"ignore\")\n"
      ],
      "metadata": {
        "id": "azjQ1G2Y1xZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train):\n",
        "    print(\"Defining features and labels...\")\n",
        "\n",
        "    # Preprocessing pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        [\n",
        "            (\"num\", StandardScaler(), numeric_features),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\", classes=np.unique(y_train), y=y_train\n",
        "    )\n",
        "\n",
        "    print(\"Preprocessing pipeline defined.\")\n",
        "\n",
        "    model = Pipeline(\n",
        "        [\n",
        "            (\"preprocessor\", preprocessor),\n",
        "            (\n",
        "                \"classifier\",\n",
        "                RandomForestClassifier(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=8,\n",
        "                    min_samples_split=5,\n",
        "                    max_features=\"sqrt\",\n",
        "                    class_weight=dict(zip(np.unique(y_train), class_weights)),\n",
        "                    random_state=42,\n",
        "                ),\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # param_grid = {\n",
        "    #     \"classifier__n_estimators\": [180, 150, 100],\n",
        "    #     \"classifier__max_depth\": [7, 8, 5],\n",
        "    #     \"classifier__min_samples_split\": [3, 5],\n",
        "    # }\n",
        "    # # Perform hyperparameter tuning using GridSearchCV\n",
        "    # grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring=\"accuracy\", verbose=10)\n",
        "    # print(\"Starting hyperparameter tuning...\")\n",
        "    # grid_search.fit(X_train, y_train)\n",
        "    # best_model = grid_search.best_estimator_\n",
        "    # print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    # print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
        "\n",
        "\n",
        "    # Generate learning curves\n",
        "    print(\"Computing learning curves...\")\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        model,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        scoring=\"accuracy\",\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    # Compute mean and std\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    val_scores_mean = np.mean(val_scores, axis=1)\n",
        "    val_scores_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_scores_mean, label=\"Training Accuracy\", marker='o')\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1)\n",
        "    plt.plot(train_sizes, val_scores_mean, label=\"Validation Accuracy\", marker='s')\n",
        "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
        "                     val_scores_mean + val_scores_std, alpha=0.1)\n",
        "    plt.title(\"Learning Curve\")\n",
        "    plt.xlabel(\"Training Set Size\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"learning_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Fit final model\n",
        "    print(\"Fitting final model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "s4BnGjGh14GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(df, name, max_nodes=100, desired_clusters=20):\n",
        "    def get_subnet(ip, mask):\n",
        "        try:\n",
        "            return str(ipaddress.ip_network(f\"{ip}/{mask}\", strict=False).network_address) + f\"/{mask}\"\n",
        "        except ValueError:\n",
        "            return \"invalid\"\n",
        "\n",
        "    # Try to pick best subnet mask based on IP distribution\n",
        "    def choose_subnet_mask(ips, desired_clusters):\n",
        "        def count_clusters(mask):\n",
        "            try:\n",
        "                return len(set(\n",
        "                    str(ipaddress.ip_network(f\"{ip}/{mask}\", strict=False).network_address)\n",
        "                    for ip in ips\n",
        "                ))\n",
        "            except ValueError:\n",
        "                return float(\"inf\")\n",
        "\n",
        "        best_mask = 24\n",
        "        best_diff = float(\"inf\")\n",
        "\n",
        "        for mask in range(16, 29):  # Test from /16 to /28\n",
        "            num_clusters = count_clusters(mask)\n",
        "            diff = abs(num_clusters - desired_clusters)\n",
        "            if diff < best_diff:\n",
        "                best_mask = mask\n",
        "                best_diff = diff\n",
        "\n",
        "        return best_mask\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    all_ips = set()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        src, dst = row[\"Source IP\"], row[\"Destination IP\"]\n",
        "        G.add_edge(src, dst)\n",
        "        all_ips.update([src, dst])\n",
        "\n",
        "    # Trim to top N active nodes\n",
        "    degrees = dict(G.degree)\n",
        "    top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:max_nodes]\n",
        "    G = G.subgraph(top_nodes).copy()\n",
        "\n",
        "    # Re-collect IPs in trimmed graph\n",
        "    trimmed_ips = list(G.nodes)\n",
        "    subnet_mask = choose_subnet_mask(trimmed_ips, desired_clusters)\n",
        "\n",
        "    # Build subnet groups\n",
        "    subnet_groups = defaultdict(set)\n",
        "    for node in G.nodes:\n",
        "        subnet = get_subnet(node, subnet_mask)\n",
        "        subnet_groups[subnet].add(node)\n",
        "\n",
        "    # Assign positions to each subnet group (cluster)\n",
        "    pos = {}\n",
        "    cluster_offset = 0\n",
        "    spacing = 5\n",
        "    for i, (subnet, nodes) in enumerate(subnet_groups.items()):\n",
        "        subgraph = G.subgraph(nodes)\n",
        "        cluster_pos = nx.spring_layout(subgraph, k=0.5, seed=42)\n",
        "        for node in cluster_pos:\n",
        "            cluster_pos[node][0] += cluster_offset\n",
        "        pos.update(cluster_pos)\n",
        "        cluster_offset += spacing\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    nx.draw(G, pos, with_labels=True, node_size=700, node_color=\"lightblue\",\n",
        "            font_size=9, font_weight=\"bold\", edge_color=\"gray\")\n",
        "    plt.title(f\"Knowledge Graph (Subnet Clustered - /{subnet_mask})\")\n",
        "    plt.savefig(f\"knowledge_graph_{name}.png\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "bXZRdMFL2Uo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_data(csv_files)\n",
        "\n",
        "# shuffle the data\n",
        "# df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Total rows after concatenation: {len(df)}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"Preprocessing data...\")\n",
        "df, encoder = preprocess_data(df)\n",
        "print(\"Data preprocessing complete.\")\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp1OcGMv2YJP",
        "outputId": "3814e689-009b-445c-d6b0-16ce95695304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets/Friday-WorkingHours-Morning.pcap_ISCX.csv...\n",
            "Loaded datasets/Friday-WorkingHours-Morning.pcap_ISCX.csv with 191033 rows.\n",
            "Loading datasets/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...\n",
            "Loaded datasets/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv with 225745 rows.\n",
            "Loading datasets/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv...\n",
            "Loaded datasets/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv with 288602 rows.\n",
            "Loading datasets/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...\n",
            "Loaded datasets/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv with 286467 rows.\n",
            "Loading datasets/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv...\n",
            "Loaded datasets/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv with 458968 rows.\n",
            "Loading datasets/Wednesday-workingHours.pcap_ISCX.csv...\n",
            "Loaded datasets/Wednesday-workingHours.pcap_ISCX.csv with 208788 rows.\n",
            "Loading datasets/Tuesday-WorkingHours.pcap_ISCX.csv...\n",
            "Loaded datasets/Tuesday-WorkingHours.pcap_ISCX.csv with 248813 rows.\n",
            "Loading datasets/Monday-WorkingHours.pcap_ISCX.csv...\n",
            "Loaded datasets/Monday-WorkingHours.pcap_ISCX.csv with 529918 rows.\n",
            "Total rows after concatenation: 2148802\n",
            "Columns: ['Flow ID', 'Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n",
            "                                   Flow ID      Source IP  Source Port  \\\n",
            "0  192.168.10.3-192.168.10.50-3268-56108-6  192.168.10.50      56108.0   \n",
            "1   192.168.10.3-192.168.10.50-389-42144-6  192.168.10.50      42144.0   \n",
            "2                    8.0.6.4-8.6.0.1-0-0-0        8.6.0.1          0.0   \n",
            "3   192.168.10.9-224.0.0.252-63210-5355-17   192.168.10.9      63210.0   \n",
            "4            192.168.10.9-224.0.0.22-0-0-0   192.168.10.9          0.0   \n",
            "\n",
            "  Destination IP  Destination Port  Protocol      Timestamp  Flow Duration  \\\n",
            "0   192.168.10.3            3268.0       6.0  7/7/2017 8:59    112740690.0   \n",
            "1   192.168.10.3             389.0       6.0  7/7/2017 8:59    112740560.0   \n",
            "2        8.0.6.4               0.0       0.0  7/7/2017 9:00    113757377.0   \n",
            "3    224.0.0.252            5355.0      17.0  7/7/2017 9:00       100126.0   \n",
            "4     224.0.0.22               0.0       0.0  7/7/2017 9:00        54760.0   \n",
            "\n",
            "   Total Fwd Packets  Total Backward Packets  ...  min_seg_size_forward  \\\n",
            "0               32.0                    16.0  ...                  32.0   \n",
            "1               32.0                    16.0  ...                  32.0   \n",
            "2              545.0                     0.0  ...                   0.0   \n",
            "3               22.0                     0.0  ...                  32.0   \n",
            "4                4.0                     0.0  ...                   0.0   \n",
            "\n",
            "    Active Mean    Active Std  Active Max  Active Min   Idle Mean  \\\n",
            "0  3.594286e+02  1.199802e+01       380.0       343.0  16100000.0   \n",
            "1  3.202857e+02  1.574499e+01       330.0       285.0  16100000.0   \n",
            "2  9.361829e+06  7.324646e+06  18900000.0        19.0  12200000.0   \n",
            "3  0.000000e+00  0.000000e+00         0.0         0.0         0.0   \n",
            "4  0.000000e+00  0.000000e+00         0.0         0.0         0.0   \n",
            "\n",
            "       Idle Std    Idle Max    Idle Min   Label  \n",
            "0  4.988048e+05  16400000.0  15400000.0  BENIGN  \n",
            "1  4.987937e+05  16400000.0  15400000.0  BENIGN  \n",
            "2  6.935824e+06  20800000.0   5504997.0  BENIGN  \n",
            "3  0.000000e+00         0.0         0.0  BENIGN  \n",
            "4  0.000000e+00         0.0         0.0  BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "Preprocessing data...\n",
            "Starting preprocessing...\n",
            "Converting Timestamp...\n",
            "Timestamp conversion complete.\n",
            "Categorizing IP addresses...\n",
            "IP categorization complete.\n",
            "Entering categorical columns...\n",
            "Categorical columns entered.\n",
            "Data preprocessing complete.\n",
            "                                   Flow ID      Source IP  Source Port  \\\n",
            "0  192.168.10.3-192.168.10.50-3268-56108-6  192.168.10.50      56108.0   \n",
            "1   192.168.10.3-192.168.10.50-389-42144-6  192.168.10.50      42144.0   \n",
            "2                    8.0.6.4-8.6.0.1-0-0-0        8.6.0.1          0.0   \n",
            "3   192.168.10.9-224.0.0.252-63210-5355-17   192.168.10.9      63210.0   \n",
            "4            192.168.10.9-224.0.0.22-0-0-0   192.168.10.9          0.0   \n",
            "\n",
            "  Destination IP  Destination Port  Protocol           Timestamp  \\\n",
            "0   192.168.10.3            3268.0       6.0 2017-07-07 08:59:00   \n",
            "1   192.168.10.3             389.0       6.0 2017-07-07 08:59:00   \n",
            "2        8.0.6.4               0.0       0.0 2017-07-07 09:00:00   \n",
            "3    224.0.0.252            5355.0      17.0 2017-07-07 09:00:00   \n",
            "4     224.0.0.22               0.0       0.0 2017-07-07 09:00:00   \n",
            "\n",
            "   Flow Duration  Total Fwd Packets  Total Backward Packets  ...    Idle Max  \\\n",
            "0    112740690.0               32.0                    16.0  ...  16400000.0   \n",
            "1    112740560.0               32.0                    16.0  ...  16400000.0   \n",
            "2    113757377.0              545.0                     0.0  ...  20800000.0   \n",
            "3       100126.0               22.0                     0.0  ...         0.0   \n",
            "4        54760.0                4.0                     0.0  ...         0.0   \n",
            "\n",
            "     Idle Min   Label  Source_Internal  Dest_Internal  Hour  DayOfWeek  \\\n",
            "0  15400000.0  BENIGN                1              1   8.0        4.0   \n",
            "1  15400000.0  BENIGN                1              1   8.0        4.0   \n",
            "2   5504997.0  BENIGN                0              0   9.0        4.0   \n",
            "3         0.0  BENIGN                1              0   9.0        4.0   \n",
            "4         0.0  BENIGN                1              0   9.0        4.0   \n",
            "\n",
            "    source_ip  destination_ip  label_encoded  \n",
            "0  3232238130      3232238083              0  \n",
            "1  3232238130      3232238083              0  \n",
            "2   134610945       134219268              0  \n",
            "3  3232238089      3758096636              0  \n",
            "4  3232238089      3758096406              0  \n",
            "\n",
            "[5 rows x 92 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.4, random_state=42, stratify=df[\"label_encoded\"]\n",
        ")\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(data=train_df, x=\"label_encoded\", bins=30, kde=True)\n",
        "# plt.title(\"Distribution of Encoded Labels\")\n",
        "# plt.xlabel(\"Encoded Labels\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "\n",
        "# initialize_database()\n",
        "# store_in_database(train_df)\n",
        "\n",
        "train_ips = train_df[\"Source IP\"].unique()\n",
        "graph_features = calculate_graph_features(ip_filter=train_ips)"
      ],
      "metadata": {
        "id": "VldVsGlI5DCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns = numeric_features + categorical_features\n",
        "\n",
        "# Enrich datasets\n",
        "X_train = enrich_data(train_df, graph_features)\n",
        "X_test = enrich_data(test_df, graph_features)\n",
        "\n",
        "# without enrichment\n",
        "# X_train = train_df.copy()\n",
        "# X_test = test_df.copy()\n",
        "\n",
        "# print(\"Data enrichment complete.\")\n",
        "# print(\"X_train\")\n",
        "# print(X_train.head())\n",
        "# print(\"X_test\")\n",
        "# print(X_test.head())\n",
        "\n",
        "y_train = X_train[\"label_encoded\"]\n",
        "y_test = X_test[\"label_encoded\"]\n",
        "X_train = X_train.drop(columns=[\"label_encoded\"])\n",
        "X_test = X_test.drop(columns=[\"label_encoded\"])\n",
        "\n",
        "model = train_model(X_train[feature_columns], y_train)\n",
        "print(\"Model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0KVG7XA5MPi",
        "outputId": "9d29648a-e1c0-4a2e-dc28-855dab99334f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 1\n",
            "Processed chunk 2\n",
            "Processed chunk 3\n",
            "Processed chunk 4\n",
            "Processed chunk 5\n",
            "Processed chunk 6\n",
            "Processed chunk 7\n",
            "Processed chunk 8\n",
            "Processed chunk 9\n",
            "Processed chunk 10\n",
            "Processed chunk 11\n",
            "Processed chunk 12\n",
            "Processed chunk 13\n",
            "Processed chunk 14\n",
            "Processed chunk 15\n",
            "Processed chunk 16\n",
            "Processed chunk 17\n",
            "Processed chunk 18\n",
            "Processed chunk 19\n",
            "Processed chunk 20\n",
            "Processed chunk 21\n",
            "Processed chunk 22\n",
            "Processed chunk 23\n",
            "Processed chunk 24\n",
            "Processed chunk 25\n",
            "Processed chunk 26\n",
            "Processed chunk 27\n",
            "Processed chunk 28\n",
            "Processed chunk 29\n",
            "Processed chunk 30\n",
            "Processed chunk 31\n",
            "Processed chunk 32\n",
            "Processed chunk 33\n",
            "Processed chunk 34\n",
            "Processed chunk 35\n",
            "Processed chunk 36\n",
            "Processed chunk 37\n",
            "Processed chunk 38\n",
            "Processed chunk 39\n",
            "Processed chunk 40\n",
            "Processed chunk 41\n",
            "Processed chunk 42\n",
            "Processed chunk 43\n",
            "Processed chunk 44\n",
            "Processed chunk 45\n",
            "Processed chunk 46\n",
            "Processed chunk 47\n",
            "Processed chunk 48\n",
            "Processed chunk 49\n",
            "Processed chunk 50\n",
            "Processed chunk 51\n",
            "Processed chunk 52\n",
            "Processed chunk 53\n",
            "Processed chunk 54\n",
            "Processed chunk 55\n",
            "Processed chunk 56\n",
            "Processed chunk 57\n",
            "Processed chunk 58\n",
            "Processed chunk 59\n",
            "Processed chunk 60\n",
            "Processed chunk 61\n",
            "Processed chunk 62\n",
            "Processed chunk 63\n",
            "Processed chunk 64\n",
            "Processed chunk 65\n",
            "Processed chunk 66\n",
            "Processed chunk 67\n",
            "Processed chunk 68\n",
            "Processed chunk 69\n",
            "Processed chunk 70\n",
            "Processed chunk 71\n",
            "Processed chunk 72\n",
            "Processed chunk 73\n",
            "Processed chunk 74\n",
            "Processed chunk 75\n",
            "Processed chunk 76\n",
            "Processed chunk 77\n",
            "Processed chunk 78\n",
            "Processed chunk 79\n",
            "Processed chunk 80\n",
            "Processed chunk 81\n",
            "Processed chunk 82\n",
            "Processed chunk 83\n",
            "Processed chunk 84\n",
            "Processed chunk 85\n",
            "Processed chunk 86\n",
            "Processed chunk 87\n",
            "Processed chunk 88\n",
            "Processed chunk 89\n",
            "Processed chunk 90\n",
            "Processed chunk 91\n",
            "Processed chunk 92\n",
            "Processed chunk 93\n",
            "Processed chunk 94\n",
            "Processed chunk 95\n",
            "Processed chunk 96\n",
            "Processed chunk 97\n",
            "Processed chunk 98\n",
            "Processed chunk 99\n",
            "Processed chunk 100\n",
            "Processed chunk 101\n",
            "Processed chunk 102\n",
            "Processed chunk 103\n",
            "Processed chunk 104\n",
            "Processed chunk 105\n",
            "Processed chunk 106\n",
            "Processed chunk 107\n",
            "Processed chunk 108\n",
            "Processed chunk 109\n",
            "Processed chunk 110\n",
            "Processed chunk 111\n",
            "Processed chunk 112\n",
            "Processed chunk 113\n",
            "Processed chunk 114\n",
            "Processed chunk 115\n",
            "Processed chunk 116\n",
            "Processed chunk 117\n",
            "Processed chunk 118\n",
            "Processed chunk 119\n",
            "Processed chunk 120\n",
            "Processed chunk 121\n",
            "Processed chunk 122\n",
            "Processed chunk 123\n",
            "Processed chunk 124\n",
            "Processed chunk 125\n",
            "Processed chunk 126\n",
            "Processed chunk 127\n",
            "Processed chunk 128\n",
            "Processed chunk 129\n",
            "Processed chunk 1\n",
            "Processed chunk 2\n",
            "Processed chunk 3\n",
            "Processed chunk 4\n",
            "Processed chunk 5\n",
            "Processed chunk 6\n",
            "Processed chunk 7\n",
            "Processed chunk 8\n",
            "Processed chunk 9\n",
            "Processed chunk 10\n",
            "Processed chunk 11\n",
            "Processed chunk 12\n",
            "Processed chunk 13\n",
            "Processed chunk 14\n",
            "Processed chunk 15\n",
            "Processed chunk 16\n",
            "Processed chunk 17\n",
            "Processed chunk 18\n",
            "Processed chunk 19\n",
            "Processed chunk 20\n",
            "Processed chunk 21\n",
            "Processed chunk 22\n",
            "Processed chunk 23\n",
            "Processed chunk 24\n",
            "Processed chunk 25\n",
            "Processed chunk 26\n",
            "Processed chunk 27\n",
            "Processed chunk 28\n",
            "Processed chunk 29\n",
            "Processed chunk 30\n",
            "Processed chunk 31\n",
            "Processed chunk 32\n",
            "Processed chunk 33\n",
            "Processed chunk 34\n",
            "Processed chunk 35\n",
            "Processed chunk 36\n",
            "Processed chunk 37\n",
            "Processed chunk 38\n",
            "Processed chunk 39\n",
            "Processed chunk 40\n",
            "Processed chunk 41\n",
            "Processed chunk 42\n",
            "Processed chunk 43\n",
            "Processed chunk 44\n",
            "Processed chunk 45\n",
            "Processed chunk 46\n",
            "Processed chunk 47\n",
            "Processed chunk 48\n",
            "Processed chunk 49\n",
            "Processed chunk 50\n",
            "Processed chunk 51\n",
            "Processed chunk 52\n",
            "Processed chunk 53\n",
            "Processed chunk 54\n",
            "Processed chunk 55\n",
            "Processed chunk 56\n",
            "Processed chunk 57\n",
            "Processed chunk 58\n",
            "Processed chunk 59\n",
            "Processed chunk 60\n",
            "Processed chunk 61\n",
            "Processed chunk 62\n",
            "Processed chunk 63\n",
            "Processed chunk 64\n",
            "Processed chunk 65\n",
            "Processed chunk 66\n",
            "Processed chunk 67\n",
            "Processed chunk 68\n",
            "Processed chunk 69\n",
            "Processed chunk 70\n",
            "Processed chunk 71\n",
            "Processed chunk 72\n",
            "Processed chunk 73\n",
            "Processed chunk 74\n",
            "Processed chunk 75\n",
            "Processed chunk 76\n",
            "Processed chunk 77\n",
            "Processed chunk 78\n",
            "Processed chunk 79\n",
            "Processed chunk 80\n",
            "Processed chunk 81\n",
            "Processed chunk 82\n",
            "Processed chunk 83\n",
            "Processed chunk 84\n",
            "Processed chunk 85\n",
            "Processed chunk 86\n",
            "Defining features and labels...\n",
            "Preprocessing pipeline defined.\n",
            "Computing learning curves...\n",
            "Fitting final model...\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Evaluate performance on training data (before validation)\n",
        "  # print(\"\\n===== TRAINING PERFORMANCE (BEFORE VALIDATION) =====\")\n",
        "  y_train_pred = model.predict(X_train[feature_columns])\n",
        "  y_train_proba = model.predict_proba(X_train[feature_columns])\n",
        "\n",
        "  print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
        "  print(\"Training F1 Score:\", f1_score(y_train, y_train_pred, average=\"weighted\"))\n",
        "  print(\"Training ROC AUC Score:\", roc_auc_score(y_train, y_train_proba, multi_class=\"ovr\"))\n",
        "  print(\"Training Log Loss:\", log_loss(y_train, y_train_proba))\n",
        "  print(\"Training Average Precision Score:\", average_precision_score(y_train, y_train_proba))\n",
        "  print(\"Training Classification Report:\")\n",
        "  print(classification_report(y_train, y_train_pred))\n",
        "  print(\"Training Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_train, y_train_pred))\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  classes = model.classes_\n",
        "  for i, class_name in enumerate(classes):\n",
        "      precision, recall, _ = precision_recall_curve((y_train == class_name).astype(int), y_train_proba[:, i])\n",
        "      average_precision = average_precision_score((y_train == class_name).astype(int), y_train_proba[:, i])\n",
        "      plt.plot(recall, precision, label=f\"{class_name} (AP={average_precision:.2f})\")\n",
        "\n",
        "  plt.xlabel(\"Recall\")\n",
        "  plt.ylabel(\"Precision\")\n",
        "  plt.title(\"Precision-Recall Curve\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"precision_recall_curve_before_validation.png\")\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  y_train_bin = label_binarize(y_train, classes=classes)\n",
        "  for i, class_name in enumerate(classes):\n",
        "      fpr, tpr, _ = roc_curve(y_train_bin[:, i], y_train_proba[:, i])\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      plt.plot(fpr, tpr, label=f\"{class_name} (AUC={roc_auc:.2f})\")\n",
        "  plt.plot([0, 1], [0, 1], \"k--\")\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"ROC Curve\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"roc_curve_before_validation.png\")\n",
        "  plt.close()\n",
        "\n",
        "  # Plot confusion matrix\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  # Get the original class names from the encoder\n",
        "  class_names = encoder.classes_\n",
        "\n",
        "  # Create confusion matrix with numeric indices\n",
        "  cm = confusion_matrix(y_train, y_train_pred)\n",
        "\n",
        "  # Plot using class names for labels\n",
        "  sns.heatmap(\n",
        "      cm,\n",
        "      annot=True,\n",
        "      fmt=\"d\",\n",
        "      cmap=\"Blues\",\n",
        "      cbar=False,\n",
        "      xticklabels=class_names,\n",
        "      yticklabels=class_names,\n",
        "  )\n",
        "  plt.xlabel(\"Predicted\")\n",
        "  plt.ylabel(\"True\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.tight_layout()  # Ensure labels fit properly\n",
        "  plt.savefig(\"confusion_matrix_before_validation.png\")\n",
        "  plt.close()\n",
        "\n",
        "  # load model from file\n",
        "  # import joblib\n",
        "  # model = joblib.load(\"threat_model.pkl\")\n",
        "\n",
        "  print(\"\\n===== TEST PERFORMANCE (AFTER VALIDATION) =====\")\n",
        "  print(\"Evaluating model...\")\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_proba = model.predict_proba(X_test)\n",
        "  print(\"Model evaluation complete.\")\n",
        "\n",
        "  # dump the model to a file\n",
        "  import joblib\n",
        "\n",
        "  joblib.dump(model, \"threat_model.pkl\")\n",
        "  print(\"Model saved as threat_model.pkl\")\n",
        "\n",
        "  print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  print(\"F1 Score:\", f1_score(y_test, y_pred, average=\"weighted\"))\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba, multi_class=\"ovr\"))\n",
        "  print(\"Log Loss:\", log_loss(y_test, y_proba))\n",
        "  print(\"Average Precision Score:\", average_precision_score(y_test, y_proba))\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  import seaborn as sns\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  classes = model.classes_\n",
        "  for i, class_name in enumerate(classes):\n",
        "      precision, recall, _ = precision_recall_curve((y_test == class_name).astype(int), y_proba[:, i])\n",
        "      average_precision = average_precision_score((y_test == class_name).astype(int), y_proba[:, i])\n",
        "      plt.plot(recall, precision, label=f\"{class_name} (AP={average_precision:.2f})\")\n",
        "  plt.xlabel(\"Recall\")\n",
        "  plt.ylabel(\"Precision\")\n",
        "  plt.title(\"Precision-Recall Curve\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"precision_recall_curve_after_validation.png\")\n",
        "  plt.close()\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  y_test_bin = label_binarize(y_test, classes=classes)\n",
        "  for i, class_name in enumerate(classes):\n",
        "      fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      plt.plot(fpr, tpr, label=f\"{class_name} (AUC={roc_auc:.2f})\")\n",
        "  plt.plot([0, 1], [0, 1], \"k--\")\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"ROC Curve\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"roc_curve_after_validation.png\")\n",
        "  plt.close()\n",
        "\n",
        "  # Plot confusion matrix\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  # Get the original class names from the encoder\n",
        "  class_names = encoder.classes_\n",
        "\n",
        "  # Create confusion matrix with numeric indices\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  # Plot using class names for labels\n",
        "  sns.heatmap(\n",
        "      cm,\n",
        "      annot=True,\n",
        "      fmt=\"d\",\n",
        "      cmap=\"Blues\",\n",
        "      cbar=False,\n",
        "      xticklabels=class_names,\n",
        "      yticklabels=class_names,\n",
        "  )\n",
        "  plt.xlabel(\"Predicted\")\n",
        "  plt.ylabel(\"True\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.tight_layout()  # Ensure labels fit properly\n",
        "  plt.savefig(\"confusion_matrix_after_validation.png\")\n",
        "  plt.close()\n",
        "\n",
        "  # Add feature importance analysis\n",
        "  print(\"\\n===== FEATURE IMPORTANCE ANALYSIS =====\")\n",
        "  importances = model.named_steps[\"classifier\"].feature_importances_\n",
        "\n",
        "  # Get feature names from preprocessor\n",
        "  num_features = numeric_features\n",
        "  cat_features = model.named_steps[\"preprocessor\"].transformers_[1][1].get_feature_names_out(categorical_features).tolist()\n",
        "  feature_names = num_features + cat_features\n",
        "\n",
        "  # Create DataFrame of feature importances\n",
        "  feature_importances = pd.DataFrame({\n",
        "      \"Feature\": feature_names,\n",
        "      \"Importance\": importances,\n",
        "  }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "  print(\"Feature importances:\")\n",
        "  print(feature_importances)\n",
        "  print(\"Feature importance analysis complete.\")\n",
        "\n",
        "  # Visualize feature importance\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importances.head(15))\n",
        "  plt.title(\"Top 15 Most Important Features\")\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(\"feature_importance.png\")\n",
        "  plt.close()\n",
        "\n",
        "  # Per-class performance comparison\n",
        "  print(\"\\n===== PER-CLASS PERFORMANCE COMPARISON =====\")\n",
        "  # Training data class performance\n",
        "  train_report = classification_report(y_train, y_train_pred, output_dict=True)\n",
        "  test_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "  # Convert to DataFrames for easier handling\n",
        "  train_df = pd.DataFrame(train_report).transpose()\n",
        "  test_df = pd.DataFrame(test_report).transpose()\n",
        "\n",
        "  # Filter to keep only class data (remove accuracy, macro avg, weighted avg)\n",
        "  train_classes = train_df.iloc[:-3].copy()\n",
        "  test_classes = test_df.iloc[:-3].copy()\n",
        "\n",
        "  # Add dataset identifier\n",
        "  train_classes['dataset'] = 'Training'\n",
        "  test_classes['dataset'] = 'Test'\n",
        "\n",
        "  # Combine for plotting\n",
        "  combined_classes = pd.concat([train_classes, test_classes])\n",
        "  combined_classes = combined_classes.reset_index().rename(columns={'index': 'class'})\n",
        "\n",
        "  # Map numeric class labels to actual label names\n",
        "  class_name_mapping = dict(zip(range(len(encoder.classes_)), encoder.classes_))\n",
        "  combined_classes['class_name'] = combined_classes['class'].map(lambda x: class_name_mapping.get(int(x), x))\n",
        "\n",
        "  # Plot F1 scores per class using actual label names\n",
        "  plt.figure(figsize=(14, 8))\n",
        "  sns.barplot(x='class_name', y='f1-score', hue='dataset', data=combined_classes)\n",
        "  plt.title('F1 Score by Class: Training vs Test')\n",
        "  plt.xlabel('Class')\n",
        "  plt.ylabel('F1 Score')\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('f1_score_by_class.png')\n",
        "  plt.close()\n",
        "\n",
        "  # Print detailed per-class comparison with actual label names\n",
        "  print(\"\\nPer-class F1 scores:\")\n",
        "  class_comparison = pd.DataFrame({\n",
        "      'Class': [class_name_mapping.get(int(c), c) for c in combined_classes['class'].unique()[:len(classes)]],\n",
        "      'Training F1': train_classes['f1-score'].values,\n",
        "      'Test F1': test_classes['f1-score'].values,\n",
        "      'Difference': train_classes['f1-score'].values - test_classes['f1-score'].values\n",
        "  }).sort_values(by='Difference', ascending=False)\n",
        "  print(class_comparison)"
      ],
      "metadata": {
        "id": "iylh8RqW5R45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98585dc4-c37b-4a7a-d462-b9c03a5fc3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9945015865431973\n",
            "Training F1 Score: 0.995491470332102\n",
            "Training ROC AUC Score: 0.9999818947213694\n",
            "Training Log Loss: 0.06751720199043325\n",
            "Training Average Precision Score: 0.9968999537207119\n",
            "Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00   1025426\n",
            "           1       0.70      1.00      0.82      1180\n",
            "           2       1.00      1.00      1.00     76816\n",
            "           3       1.00      0.92      0.96     75847\n",
            "           4       0.36      1.00      0.53      3299\n",
            "           5       0.85      0.94      0.89      3478\n",
            "           6       1.00      1.00      1.00      4763\n",
            "           7       0.92      1.00      0.96        22\n",
            "           8       1.00      1.00      1.00     95358\n",
            "           9       0.96      1.00      0.98      1784\n",
            "          10       0.99      1.00      0.99       904\n",
            "          11       0.87      1.00      0.93        13\n",
            "          12       0.99      0.99      0.99       391\n",
            "\n",
            "    accuracy                           0.99   1289281\n",
            "   macro avg       0.89      0.99      0.93   1289281\n",
            "weighted avg       1.00      0.99      1.00   1289281\n",
            "\n",
            "Training Confusion Matrix:\n",
            "[[1024880     507      23       0       0       0       0       2       4\n",
            "        0      10       0       0]\n",
            " [      0    1180       0       0       0       0       0       0       0\n",
            "        0       0       0       0]\n",
            " [      3       0   76806       0       0       0       0       0       7\n",
            "        0       0       0       0]\n",
            " [      0       0       0   69719    5549     579       0       0       0\n",
            "        0       0       0       0]\n",
            " [      0       0       0       2    3297       0       0       0       0\n",
            "        0       0       0       0]\n",
            " [      0       0       0       2     203    3273       0       0       0\n",
            "        0       0       0       0]\n",
            " [      0       0       0       0       0       0    4763       0       0\n",
            "        0       0       0       0]\n",
            " [      0       0       0       0       0       0       0      22       0\n",
            "        0       0       0       0]\n",
            " [      0       0     108       0       0       0      16       0   95166\n",
            "       68       0       0       0]\n",
            " [      0       0       0       0       0       0       0       0       0\n",
            "     1784       0       0       0]\n",
            " [      0       0       0       0       0       0       0       0       0\n",
            "        0     900       0       4]\n",
            " [      0       0       0       0       0       0       0       0       0\n",
            "        0       0      13       0]\n",
            " [      0       0       0       0       0       0       0       0       0\n",
            "        0       0       2     389]]\n",
            "\n",
            "===== TEST PERFORMANCE (AFTER VALIDATION) =====\n",
            "Evaluating model...\n",
            "Model evaluation complete.\n",
            "Model saved as threat_model.pkl\n",
            "Test Accuracy: 0.9946900657459212\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    683618\n",
            "           1       0.69      1.00      0.82       786\n",
            "           2       1.00      1.00      1.00     51211\n",
            "           3       1.00      0.92      0.96     50565\n",
            "           4       0.37      1.00      0.54      2200\n",
            "           5       0.85      0.93      0.89      2318\n",
            "           6       1.00      1.00      1.00      3175\n",
            "           7       1.00      1.00      1.00        14\n",
            "           8       1.00      1.00      1.00     63572\n",
            "           9       0.96      1.00      0.98      1190\n",
            "          10       1.00      1.00      1.00       603\n",
            "          11       0.89      1.00      0.94         8\n",
            "          12       0.99      1.00      0.99       261\n",
            "\n",
            "    accuracy                           0.99    859521\n",
            "   macro avg       0.90      0.99      0.93    859521\n",
            "weighted avg       1.00      0.99      1.00    859521\n",
            "\n",
            "F1 Score: 0.9956233514070438\n",
            "Confusion Matrix:\n",
            "[[683257    352      6      0      0      0      0      0      2      0\n",
            "       1      0      0]\n",
            " [     0    786      0      0      0      0      0      0      0      0\n",
            "       0      0      0]\n",
            " [     0      0  51204      0      0      0      0      0      7      0\n",
            "       0      0      0]\n",
            " [     0      0      0  46672   3511    382      0      0      0      0\n",
            "       0      0      0]\n",
            " [     0      0      0      2   2198      0      0      0      0      0\n",
            "       0      0      0]\n",
            " [     0      0      0      1    160   2157      0      0      0      0\n",
            "       0      0      0]\n",
            " [     0      0      1      0      0      0   3174      0      0      0\n",
            "       0      0      0]\n",
            " [     0      0      0      0      0      0      0     14      0      0\n",
            "       0      0      0]\n",
            " [     0      0     72      0      0      0     12      0  63437     51\n",
            "       0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0   1190\n",
            "       0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "     600      0      3]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      8      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      1    260]]\n",
            "ROC AUC Score: 0.9999794469994515\n",
            "Log Loss: 0.07711100926336145\n",
            "Average Precision Score: 0.9942304223129532\n",
            "\n",
            "===== FEATURE IMPORTANCE ANALYSIS =====\n",
            "Feature importances:\n",
            "                   Feature    Importance\n",
            "12        Destination Port  1.209527e-01\n",
            "0            Flow Duration  8.159052e-02\n",
            "27           DayOfWeek_2.0  8.126910e-02\n",
            "11             Source Port  7.345257e-02\n",
            "2   Total Backward Packets  7.062534e-02\n",
            "29           DayOfWeek_6.0  6.592227e-02\n",
            "23               Hour_10.0  6.381432e-02\n",
            "28           DayOfWeek_4.0  6.019224e-02\n",
            "22                Hour_9.0  5.859212e-02\n",
            "1        Total Fwd Packets  5.167175e-02\n",
            "3               out_degree  3.797992e-02\n",
            "19                Hour_4.0  3.321474e-02\n",
            "10          destination_ip  2.986522e-02\n",
            "17                Hour_2.0  2.967027e-02\n",
            "8     days_since_last_seen  2.578336e-02\n",
            "9                source_ip  2.435514e-02\n",
            "7        degree_centrality  1.920940e-02\n",
            "4                in_degree  1.688601e-02\n",
            "6            activity_span  1.597698e-02\n",
            "34         Dest_Internal_1  9.234675e-03\n",
            "33         Dest_Internal_0  8.410889e-03\n",
            "18                Hour_3.0  5.719122e-03\n",
            "14            Protocol_6.0  3.106464e-03\n",
            "5              active_days  2.436011e-03\n",
            "15           Protocol_17.0  2.353723e-03\n",
            "32       Source_Internal_1  2.111047e-03\n",
            "30           DayOfWeek_nan  2.040827e-03\n",
            "31       Source_Internal_0  1.994433e-03\n",
            "26                Hour_nan  1.045567e-03\n",
            "25               Hour_12.0  2.577347e-04\n",
            "24               Hour_11.0  1.804892e-04\n",
            "16                Hour_1.0  7.501267e-05\n",
            "21                Hour_8.0  5.358368e-06\n",
            "13            Protocol_0.0  4.359294e-06\n",
            "20                Hour_5.0  2.832549e-07\n",
            "Feature importance analysis complete.\n",
            "\n",
            "===== PER-CLASS PERFORMANCE COMPARISON =====\n",
            "\n",
            "Per-class F1 scores:\n",
            "                        Class  Training F1   Test F1  Difference\n",
            "1                         Bot     0.823160  0.817048    0.006112\n",
            "5               DoS slowloris     0.893042  0.888203    0.004840\n",
            "9                 SSH-Patator     0.981298  0.979021    0.002277\n",
            "6                 FTP-Patator     0.998323  0.997956    0.000367\n",
            "8                    PortScan     0.998935  0.998866    0.000068\n",
            "0                      BENIGN     0.999732  0.999736   -0.000004\n",
            "12            Web Attack  XSS     0.992347  0.992366   -0.000019\n",
            "2                        DDoS     0.999083  0.999161   -0.000078\n",
            "3                    DoS Hulk     0.957876  0.959934   -0.002058\n",
            "10    Web Attack  Brute Force     0.992282  0.996678   -0.004395\n",
            "4            DoS Slowhttptest     0.534014  0.544801   -0.010787\n",
            "11  Web Attack  Sql Injection     0.928571  0.941176   -0.012605\n",
            "7                Infiltration     0.956522  1.000000   -0.043478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation\n",
        "print(\"\\n===== CROSS-VALIDATION =====\")\n",
        "cv_scores = cross_val_score(model, X_train[feature_columns], y_train, cv=5, scoring=\"f1_weighted\", n_jobs=-1)\n",
        "print(\"Cross-validation F1 scores:\", cv_scores)\n",
        "print(\"Mean cross-validation F1 score:\", np.mean(cv_scores))\n",
        "print(\"Cross-validation complete.\")\n",
        "\n",
        "# print(\"Feature importances:\")\n",
        "# importances = model.named_steps[\"classifier\"].feature_importances_\n",
        "# feature_names = (\n",
        "#     numeric_features + model.named_steps[\"preprocessor\"].transformers_[1][1].get_feature_names_out(categorical_features).tolist()\n",
        "# )\n",
        "# feature_importances = pd.DataFrame(\n",
        "#     {\n",
        "#         \"Feature\": feature_names,\n",
        "#         \"Importance\": importances,\n",
        "#     }\n",
        "# ).sort_values(by=\"Importance\", ascending=False)\n",
        "# print(feature_importances)\n",
        "# print(\"Feature importances computed.\")"
      ],
      "metadata": {
        "id": "T3rQLBOm5dw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d1e206-6c0a-4c5b-e3e4-7d59d85f1a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== CROSS-VALIDATION =====\n",
            "Cross-validation F1 scores: [0.99685539 0.99748492 0.99762467 0.99792112 0.9959969 ]\n",
            "Mean cross-validation F1 score: 0.9971765981567728\n",
            "Cross-validation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "import joblib\n",
        "\n",
        "joblib.dump(model, \"threat_model.pkl\")\n",
        "print(\"Model saved as threat_model.pkl\")\n",
        "\n",
        "# Save the encoder\n",
        "joblib.dump(encoder, \"label_encoder.pkl\")\n",
        "print(\"Label encoder saved as label_encoder.pkl\")\n",
        "\n",
        "# Save the graph features, to a db\n",
        "# GRAPH_FEATURES_DB = \"graph_features.db\"\n",
        "# if pathlib.Path(GRAPH_FEATURES_DB).exists():\n",
        "#     pathlib.Path(GRAPH_FEATURES_DB).unlink()\n",
        "# with closing(sqlite3.connect(GRAPH_FEATURES_DB)) as conn:\n",
        "#     graph_features.to_sql(\"graph_features\", conn, if_exists=\"replace\", index=False)\n",
        "#     conn.commit()"
      ],
      "metadata": {
        "id": "5d7jkLc75WSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}